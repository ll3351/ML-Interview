# ML-Interview
机器学习面试问题总结
2018年准备找工作了，决心总结一下，问题都是网上的资源和自己面试遇到的。
## 什么是boosting tree
提升树是以分类树或回归树为基本分类器的提升方法。 提升树被认为是统计学习中性能最好的方法之一。 提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。 以决策树为基函数的提升方法称为提升树（boosting tree）。
####Boosting算法的两个核心问题：
（1）在每一轮如何改变训练数据的权值或概率分布？
AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列的弱分类器“分而治之”。

（2）如何将弱分类器组合成一个强分类器？
弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。

## GBDT
GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。它在被提出之初就和SVM一起被认为是泛化能力较强的算法。

## L1和L2正则为何可以减弱over-fitting?
目标函数 = loss + 正则
loss = 0 过拟合

## L1和L2正则有什么区别
相同点：都用于避免过拟合

不同点：
L1可以让一部分特征的系数缩小到0，从而间接实现特征选择。所以L1适用于特征之间有关联的情况。

L2让所有特征的系数都缩小，但是不会减为0，它会使优化求解稳定快速。所以L2适用于特征之间没有关联的情况

## KNN和LR有什么本质区别
LR属于线性模型。
因为 logistic 回归的决策边界（decision boundary）是线性的。
KNN属于非线性模型

## 怎么理解Dropout
在每次训练的时候使用dropout，每个神经元有一定的概率被移除，这样可以使得一个神经元的训练不依赖于另外一个神经元，同样也就使得特征之间的协同作用被减弱。Hinton认为，过拟合可以通过阻止某些特征的协同作用来缓解。增加鲁棒性。
也可以理解为相当于在训练不同的网络，最后投票来决定结果。

## 为什么random forest具有特征选择的功能
树类分类器其实都可以, 因为可以判断每个特征对应的信息增益, 信息增益越大的特征分类效果理论上来说就越好。在非结构话数据的项目中常常会用这种方法来选取有用的特征, 从而降低特征维度。在特特征维度高的时候还是特别好用的。

## random forest有哪些重要的参数？
#### A. max_features：
随机森林允许单个决策树使用特征的最大数量。 Python为最大特征数提供了多个可选项。 下面是其中的几个：
Auto/None ：简单地选取所有特征，每颗树都可以利用他们。这种情况下，每颗树都没有任何的限制。
sqrt ：此选项是每颗子树可以利用总特征数的平方根个。 例如，如果变量（特征）的总数是100，所以每颗子树只能取其中的10个。“log2”是另一种相似类型的选项。
0.2：此选项允许每个随机森林的子树可以利用变量（特征）数的20％。如果想考察的特征x％的作用， 我们可以使用“0.X”的格式。
max_features如何影响性能和速度？
增加max_features一般能提高模型的性能，因为在每个节点上，我们有更多的选择可以考虑。 然而，这未必完全是对的，因为它降低了单个树的多样性，而这正是随机森林独特的优点。 但是，可以肯定，你通过增加max_features会降低算法的速度。 因此，你需要适当的平衡和选择最佳max_features。

#### B. n_estimators：
在利用最大投票数或平均值来预测之前，你想要建立子树的数量。 较多的子树可以让模型有更好的性能，但同时让你的代码变慢。 你应该选择尽可能高的值，只要你的处理器能够承受的住，因为这使你的预测更好更稳定。

#### C. min_sample_leaf：
如果您以前编写过一个决策树，你能体会到最小样本叶片大小的重要性。 叶是决策树的末端节点。 较小的叶子使模型更容易捕捉训练数据中的噪声。 一般来说，我更偏向于将最小叶子节点数目设置为大于50。在你自己的情况中，你应该尽量尝试多种叶子大小种类，以找到最优的那个。

## DNN为什么功能强大，说说你的理解
深度学习的深度一方面增加了大量的参数，增加的参数意味着这个网络的表达能力更强大了。可以学习和区分的特征更多了。而一旦学习到的特征变多的话，我们在分类和识别的能力也就变好了。
从简单特征到抽象特征。随着网络深度增加，提取的特征不断复杂化。更能理解复杂概念。

## SVM的损失函数是什么？怎么理解
2分类SVM等于Hinge损失 + L2正则化。

## 介绍下Maxout
maxout激活函数，它具有如下性质：
1、maxout激活函数并不是一个固定的函数，不像Sigmod、Relu、Tanh等函数，是一个固定的函数方程
2、它是一个可学习的激活函数，因为我们W参数是学习变化的。
3、它是一个分段线性函数：

maxout的拟合能力是非常强的，它可以拟合任意的的凸函数。最直观的解释就是任意的凸函数都可以由分段线性函数以任意精度拟合（学过高等数学应该能明白），而maxout又是取k个隐隐含层节点的最大值，这些”隐隐含层”节点也是线性的，所以在不同的取值范围下，最大值也可以看做是分段线性的（分段的个数与k值有关）

## 项目中over-fitting了，你怎么办
1. regularization：L2用的最多，L1也有用的。
2. dropout：一大利器。
3. early stop：结合cross validation使用。
4. cross validation：当数据量较小的时候，应该是用来减轻 overfitting 最好的方式了吧。
5. 当然，尽可能的扩大 training dataset 才是王道。
6. 在训练之前记得 shuffle 一下数据集，一般是每次训练一个 epoch（就是把 training dataset 训练了一遍）后就 shuffle 一次，但是对于较大的数据集可以只 shuffle 一次，虽然这样会使得训练在第二个 epoch 就变得 biased，但是带来的好处可以 overcome 这种缺陷。

作者：小华仔
链接：https://www.zhihu.com/question/26898675/answer/151101888
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
## 详细说一个你知道的优化算法(Adam等)
## 项目(比赛）怎么做的模型的ensemble
https://www.leiphone.com/news/201710/YoMbUNqRlasrpgle.html

常见的Ensemble方法有Bagging、Boosting、Stacking、Blending。
1.6.1 Bagging

Bagging是将多个模型（基学习器）的预测结果简单地加权平均或者投票。Bagging的好处在于可以并行地训练基学习器，其中Random Forest就用到了Bagging的思想。

1.6.2 Boosting

Boosting的思想有点像知错能改，每训练一个基学习器，是为了弥补上一个基学习器所犯的错误。其中著名的算法有AdaBoost，Gradient Boost。Gradient Boost Tree就用到了这种思想。

1.6.3 Stacking

Stacking是用新的模型（次学习器）去学习怎么组合那些基学习器，它的思想源自于Stacked Generalization（http://www.machine-learning.martinsewell.com/ensembles/stacking/Wolpert1992.pdf）这篇论文。如果把Bagging看作是多个基分类器的线性组合，那么Stacking就是多个基分类器的非线性组合。Stacking可以很灵活，它可以将学习器一层一层地堆砌起来，形成一个网状的结构，如下图：

1.6.4 Blending

Blending与Stacking很类似，它们的区别可以参考这里（https://mlwave.com/kaggle-ensembling-guide/）

## stacking是什么？需要注意哪些问题
## 了解哪些online learning的算法
## 如何解决样本不均衡的问题
## fasterRCNN中的ROIPooling是如何实现的
## 如何进行特征的选择
## 如何进行模型的选择
## 什么是梯度消失？怎么解决
梯度消失问题发生时，接近于输出层的hidden layer 3等的权值更新相对正常，但前面的hidden layer 1的权值更新会变得很慢，导致前面的层权值几乎不变，仍接近于初始化的权值，这就导致hidden layer 1相当于只是一个映射层，对所有的输入做了一个同一映射，这是此深层网络的学习就等价于只有后几层的浅层网络的学习了。

其实梯度爆炸和梯度消失问题都是因为网络太深，链式求导，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑用ReLU激活函数取代sigmoid激活函数。

## 常用的有哪些损失函数
## XX用户画像挖掘怎么做的feature engineering?
## 假设一个5*5的filter与图像卷积，如何降低计算量？
## 做过模型压缩吗？介绍下
## 什么是residual learning？说说你的理解
## residual learning所说的residual和GBDT中的residual有什么区别？
## FFM和FTRL有过了解吗？
## 你对现在Deep Learning的发展和遇到的问题有什么看法？
